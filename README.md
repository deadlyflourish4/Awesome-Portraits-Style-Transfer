# Awesome-Portraits-Style-Transfer  [![Awesome](https://awesome.re/badge.svg)](https://awesome.re) 

> An archive of studies related to portrait style transfer

<img src="./content/cover.png">

## Table of Contents
+ [Papers](#Papers)
+ [Repositories](#Repositories)
+ [Datasets](#Datasets)


## Papers
### Based on GANs
+ Dongyeun Lee , Jae Young Lee , Doyeon Kim , Jaehyun Choi , Jaejun Yoo and Junmo Kim: "*Fix the Noise: Disentangling Source Feature for Controllable Domain Translation*" CVPR (2023) [[paper]](https://arxiv.org/abs/2303.11545) [[homepage]](https://github.com/LeeDongYeun/FixNoise)
+ Shuai Yang, Liming Jiang, Ziwei Liu and Chen Change Loy: "*StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces*" ICCV 2023 [[paper]](https://www.mmlab-ntu.com/project/styleganex/) [[homepage]](https://github.com/williamyang1991/StyleGANEX)
+ Shuai Yang, Liming Jiang, Ziwei Liu and Chen Change Loy: "*Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer*" CVPR (2022) [[paper]](https://arxiv.org/pdf/2203.13248.pdf) [[homepage]](https://github.com/williamyang1991/DualStyleGAN)
+ Shuai Yang , Liming Jiang , Ziwei Liu and Chen Change Loy:"*VToonify: Controllable High-Resolution Portrait Video Style Transfer*" SIGGRAPH Asia 2022 [[paper]](https://www.mmlab-ntu.com/project/vtoonify/)[[homepage]](https://github.com/williamyang1991/VToonify)
+ Yifang Men , Yuan Yao , Miaomiao Cui , Zhouhui Lian and Xuansong Xie:"*DCT-Net: Domain-Calibrated Translation for Portrait Stylization*" SIGGRAPH 2022 (TOG) [[paper]](https://menyifang.github.io/projects/DCTNet/DCTNet.html) [[homepage]](https://github.com/menyifang/DCT-Net)
+ Shuai Yang, Liming Jiang, Ziwei Liu and Chen Change Loy: "*Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer*" CVPR 2022 [[paper]](https://www.mmlab-ntu.com/project/dualstylegan/) [[homepage]](https://github.com/williamyang1991/DualStyleGAN)
+ Yichun Shi, Xiao Yang, Yangyue Wan and Xiaohui Shen: "*SemanticStyleGAN: Learning Compositional Generative Priors for Controllable Image Synthesis and Editing*" CVPR 2022 [[paper]](https://semanticstylegan.github.io/) [[homepage]](https://github.com/seasonSH/SemanticStyleGAN)
+ Hila Chefer, Sagie Benaim, Roni Paiss, Lior Wolf: "*TargetCLIP- Official PyTorch implementation of the paper Image-Based CLIP-Guided Essence Transfer*" ECCV 2022 [[paper]](https://arxiv.org/abs/2110.12427) [[homepage]](https://github.com/hila-chefer/TargetCLIP)
+ Mingcong Liu, Qiang Li, Zekui Qin, Guoxin Zhang, Pengfei Wan, Wen Zheng: "*BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation*" NeurIPS 2021 [[paper]](https://arxiv.org/abs/2110.11728) [[homepage]](https://github.com/onion-liu/BlendGAN)
+ Zheng Gu, Chuanqi Dong, Jing Huo, Wenbin Li, and Yang Gao: "*CariMe: Unpaired Caricature Generation with Multiple Exaggerations*" TMM 2021 [[paper]](https://ieeexplore.ieee.org/abstract/document/9454341) [[homepage]](https://github.com/edward3862/CariMe-pytorch)
+ Wonjong Jang , Gwangjin Ju , Yucheol Jung , Jiaolong Yang , Xin Tong and Seungyong Lee: "*StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation*" SIGGRAPH 2021 [[paper]](https://wonjongg.me/StyleCariGAN/) [[homepage]](https://github.com/wonjongg/StyleCariGAN)
+ Yuval Alaluf, Or Patashnik and Daniel Cohen-Or: "*ReStyle: A Residual-Based StyleGAN Encoder via Iterative Refinement*" ICCV (2021) [[paper]](https://yuval-alaluf.github.io/restyle-encoder/)[[homepage]](https://github.com/yuval-alaluf/restyle-encoder?tab=readme-ov-file)
+ Xuning Shao and Weidong Zhang: "*SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised Image-to-Image Translation*" ICCV 2021 [[paper]](https://arxiv.org/abs/2103.16219) [[homepage]](https://github.com/NetEase-GameAI/SPatchGAN)
+ Fei Gao, Jingjie Zhu, Zeyuan Yu, Peng Li, Tao Wang: "*Making Robots Draw A Vivid Portrait In Two Minutes*" IROS 2020 [[paper]](https://ricelll.github.io/AiSketcher/) [[homepage]](https://github.com/fei-aiart/AiSketcher)
+ Yi, Ran and Xia, Mengfei and Liu, Yong-Jin and Lai, Yu-Kun and Rosin, Paul L: "*Line Drawings for Face Portraits from Photos using Global and Local Structure based {GAN}s*" IEEE 2020 [[paper]]() [[homepage]](https://github.com/yiranran/APDrawingGAN2)
+ Jihye Back: "*Fine-Tuning StyleGAN2 For Cartoon Face Generation*" [[paper]](https://arxiv.org/abs/2106.12445) [[homepage]](https://github.com/happy-jihye/Cartoon-StyleGAN)
+ Shi, Yichun, Deb, Debayan and Jain, Anil K: "*WarpGAN: Automatic Caricature Generation*" CVPR2019 [[paper]](https://arxiv.org/abs/1811.10100) [[homepage]](https://github.com/seasonSH/WarpGAN)
+ Xun Huang and Serge Belongie: "*Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization*" ICCV 2017 (Oral) [[paper]](https://arxiv.org/abs/1703.06868) [[homepage]](https://github.com/xunhuang1995/AdaIN-style)
+ Ran Yi, Yong-Jin Liu, Yu-Kun Lai, Paul L. Rosin: "*APDrawingGAN: Generating Artistic Portrait Drawings from Face Photos with Hierarchical GANs*" CVPR 2019 [[paper]](https://openaccess.thecvf.com/content_CVPR_2019/html/Yi_APDrawingGAN_Generating_Artistic_Portrait_Drawings_From_Face_Photos_With_Hierarchical_CVPR_2019_paper.html) [[homepage]](https://github.com/yiranran/APDrawingGAN)
+  Kunhee Kim， Sanghun Park， Eunyeong Jeon， Taehun Kim， Daijin Kim: "*A Style-aware Discriminator for Controllable Image Translation*" IEEE2022 [[paper]](https://ieeexplore.ieee.org/document/9880454) [[homepage]](https://github.com/kunheek/style-aware-discriminator)
+ Jianhui Li, Jianmin Li, Haoji Zhang, Shilong Liu, Zhengyi Wang, Zihao Xiao, Kaiwen Zheng, Jun Zhu: "*PREIM3D: 3D Consistent Precise Image Attribute Editing from a Single Image*" IEEE2023 [[paper]](https://ieeexplore.ieee.org/document/10204458) [[homepage]](https://github.com/mybabyyh/Preim3D)
+ Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy: "*Unsupervised Image-to-Image Translation with Generative Prior*" CVPR2022 [[paper]](https://www.mmlab-ntu.com/project/gpunit/) [[homepage]](https://github.com/williamyang1991/GP-UNIT)
+ Jiapeng Zhu, Yujun Shen, Deli Zhao, Bolei Zhou: "*In-Domain GAN Inversion for Real Image Editing*" ECCA2020 [[paper]](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123620579-supp.pdf) [[homepage]](https://github.com/genforce/idinvert_pytorch))
+ Fei Gao, Jingjie Zhu, Zeyuan Yu, Peng Li, Tao Wang: "Making Robots Draw A Vivid Portrait In Two Minutes" IROS2020 [[paper]](https://arxiv.org/abs/2005.05526) [[homepage]](https://github.com/fei-aiart/AiSketcher?tab=readme-ov-file)
+ Jie Chen, Gang Liu & Xin Chen: "*AnimeGAN: A Novel Lightweight GAN for Photo Animation*" 2 [[paper]](https://github.com/TachibanaYoshino/AnimeGAN/blob/master/doc/Chen2020_Chapter_AnimeGAN.pdf) [[homepage]](https://github.com/TachibanaYoshino/AnimeGAN)
### Based on Diffusion models.
+ Feihong He, Gang Li, Mengyuan Zhang, Leilei Yan , Lingyu Si , Fanzhang Li: "*FreeStyle : Free Lunch for Text-guided Style Transfer using Diffusion Models*" ArXiv 2024 [[paper]](https://arxiv.org/pdf/2401.15636.pdf) [[homepage]](https://freestylefreelunch.github.io/)
+ Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang, Haibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver Deussen, Changsheng Xu: "*ProSpect: Prompt Spectrum for Attribute-Aware Personalization of Diffusion Models*" SIGGRAPH Asia 2023 [[paper]](https://dl.acm.org/doi/pdf/10.1145/3618342) [[homepage]](https://github.com/zyxElsa/ProSpect)
+ Bin Cheng, Zuhao Liu, Yunbo Peng, Yue Lin: "*General Image-to-Image Translation with One-Shot Image Guidance*" ICCV 2023 [[paper]](https://arxiv.org/abs/2307.14352) [[homepage]](https://github.com/CrystalNeuro/visual-concept-translator)
+ Shuai Yang, Yifan Zhou, Ziwei Liu, Chen Change Loy: "*Rerender A Video: Zero-Shot Text-Guided Video-to-Video Translation*" SIGGRAPH Asia 2023 [[paper]](https://arxiv.org/pdf/2306.07954.pdf) [[homepage]](https://www.mmlab-ntu.com/project/rerender/)
+ Nisha Huang, Yu-xin Zhang, Weiming Dong: "*Style-A-Video: Agile Diffusion for Arbitrary Text-based Video Style Transfer*" ArXiv 2023 [[paper]](https://arxiv.org/abs/2305.05464) [[homepage]](https://github.com/haha-lisa/Style-A-Video)
+ Serin Yang, Hyunmin Hwang, Jong Chul Ye: "*Zero-Shot Contrastive Loss for Text-Guided Diffusion Image Style Transfer*" ICCV 2023 [[paper]](https://arxiv.org/pdf/2303.08622.pdf) [[homepage]](https://github.com/YSerin/ZeCon)
+ Yuanze Lin, Yi-Wen Chen, Yi-Hsuan Tsai, Lu Jiang, Ming-Hsuan Yang: "*Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models*" TOG 2023 [[paper]](https://arxiv.org/pdf/2302.12228.pdf) [[homepage]](https://tuning-encoder.github.io/)
+ Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, Changsheng Xu: "*Inversion-Based Style Transfer with Diffusion Models*" CVPR 2023 [[paper]](https://arxiv.org/abs/2211.13203) [[homepage]](https://github.com/zyxElsa/InST)
+ Gihyun Kwon, Jong Chul Ye: "*CLIPstyler:Image Style Transfer with a Single Text Condition*" CVPR 2022 [[paper]](https://arxiv.org/abs/2112.00374) [[homepage]](https://github.com/cyclomon/CLIPstyler)

### Others
+ Jie An, Siyu Huang, Yibing Song, Dejing Dou, Wei Liu and Jiebo Luo: "*ArtFlow: Unbiased Image Style Transfer via Reversible Neural Flows*" CVPR 2021 [[paper]](https://openaccess.thecvf.com/content/CVPR2021/papers/An_ArtFlow_Unbiased_Image_Style_Transfer_via_Reversible_Neural_Flows_CVPR_2021_paper.pdf) [[homepage]](https://github.com/pkuanjie/ArtFlow)
+ Sheng, Lu and Lin, Ziyi and Shao, Jing and Wang, Xiaogang: "*Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration*" CVPR 2018 [[paper]](https://openaccess.thecvf.com/content_cvpr_2018/papers/Sheng_Avatar-Net_Multi-Scale_Zero-Shot_CVPR_2018_paper.pdf) [[homepage]](https://lucassheng.github.io/avatar-net/)
+ 1: "**" 2 [[paper]]() [[homepage]]()
 
## Repositories
+ bryandlee: "*FreeG*" [[homepage]](https://github.com/bryandlee/FreezeG)
+ TachibanaYoshino: "*AnimeGANv2*" [[homepage]](https://github.com/TachibanaYoshino/AnimeGANv2)

+ 1: "**" 2 [[paper]]() [[homepage]]()

## Datasets
